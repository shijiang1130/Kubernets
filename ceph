1.设置存储池副本数
[root]# ceph osd pool get onepool size
size: 2
[root]# ceph osd pool set onepool size 3
set pool 1 size to 1
2.打印存储池列表
ceph osd lspools

[global]#全局设置
fsid = xxxxxxxxxxxxxxx                           #集群标识ID 
mon initial members = node1, node2, node3               #初始monitor (由创建monitor命令而定)
mon host = 10.0.1.1,10.0.1.2,10.0.1.3            #monitor IP 地址
auth cluster required = cephx                    #集群认证
auth service required = cephx                           #服务认证
auth client required = cephx                            #客户端认证
osd pool default size = 2                             #默认副本数设置 默认是3
osd pool default min size = 1                           #PG 处于 degraded 状态不影响其 IO 能力,min_size是一个PG能接受IO的最小副本数
public network = 10.0.1.0/24                            #公共网络(monitorIP段) 
cluster network = 10.0.2.0/24                           #集群网络
max open files = 131072                                 #默认0#如果设置了该选项，Ceph会设置系统的max open fds

##############################################################
[mon]
mon data = /var/lib/ceph/mon/ceph-$id
mon clock drift allowed = 1                             #默认值0.05#monitor间的clock drift
mon osd min down reporters = 13                         #默认值1#向monitor报告down的最小OSD数
mon osd down out interval = 600      #默认值300      #标记一个OSD状态为down和out之前ceph等待的秒数
##############################################################
[osd]
osd data = /var/lib/ceph/osd/ceph-$id
osd journal size = 20000 #默认5120                          #osd journal大小
osd journal = /var/lib/ceph/osd/$cluster-$id/journal    #osd journal 位置
osd mkfs type = xfs                                     #格式化系统类型
osd max write size = 512 #默认值90                   #OSD一次可写入的最大值(MB)
osd client message size cap = 2147483648   #默认值100    #客户端允许在内存中的最大数据(bytes)
osd deep scrub stride = 131072                      #默认值524288         #在Deep Scrub时候允许读取的字节数(bytes)
osd op threads = 16                                         #默认值2                   #并发文件系统操作数
osd disk threads = 4                                         #默认值1                   #OSD密集型操作例如恢复和Scrubbing时的线程
osd map cache size = 1024                              #默认值500                 #保留OSD Map的缓存(MB)
osd map cache bl size = 128                            #默认值50                #OSD进程在内存中的OSD Map缓存(MB)
osd mount options xfs = "rw,noexec,nodev,noatime,nodiratime,nobarrier"   #默认值rw,noatime,inode64  #Ceph OSD xfs Mount选项
osd recovery op priority = 2                 #默认值10              #恢复操作优先级，取值1-63，值越高占用资源越高
osd recovery max active = 10              #默认值15              #同一时间内活跃的恢复请求数 
osd max backfills = 4                           #默认值10                  #一个OSD允许的最大backfills数
osd min pg log entries = 30000           #默认值3000           #修建PGLog是保留的最大PGLog数
osd max pg log entries = 100000         #默认值10000         #修建PGLog是保留的最大PGLog数
osd mon heartbeat interval = 40           #默认值30            #OSD ping一个monitor的时间间隔（默认30s）
ms dispatch throttle bytes = 1048576000 #默认值 104857600 #等待派遣的最大消息数
objecter inflight ops = 819200                   #默认值1024           #客户端流控，允许的最大未发送io请求数，超过阀值会堵塞应用io，为0表示不受限
osd op log threshold = 50                            #默认值5                  #一次显示多少操作的log
osd crush chooseleaf type = 0                       #默认值为1              #CRUSH规则用到chooseleaf时的bucket的类型
filestore xattr use omap = true                         #默认false#为XATTRS使用object map，EXT4文件系统时使用，XFS或者btrfs也可以使用
filestore min sync interval = 10                          #默认0.1#从日志到数据盘最小同步间隔(seconds)
filestore max sync interval = 15                          #默认5#从日志到数据盘最大同步间隔(seconds)
filestore queue max ops = 25000                        #默认500#数据盘最大接受的操作数
filestore queue max bytes = 1048576000            #默认100   #数据盘一次操作最大字节数(bytes
filestore queue committing max ops = 50000       #默认500     #数据盘能够commit的操作数
filestore queue committing max bytes = 10485760000 #默认100 #数据盘能够commit的最大字节数(bytes)
filestore split multiple = 8                                               #默认值2         #前一个子目录分裂成子目录中的文件的最大数量
filestore merge threshold = 40                                        #默认值10       #前一个子类目录中的文件合并到父类的最小数量
filestore fd cache size = 1024                                         #默认值128              #对象文件句柄缓存大小
filestore op threads = 32                                                  #默认值2                    #并发文件系统操作数
journal max write bytes = 1073714824                           #默认值1048560    #journal一次性写入的最大字节数(bytes)
journal max write entries = 10000                                   #默认值100         #journal一次性写入的最大记录数
journal queue max ops = 50000                                      #默认值50            #journal一次性最大在队列中的操作数
journal queue max bytes = 10485760000                       #默认值33554432   #journal一次性最大在队列中的字节数(bytes)
##############################################################
[client]
rbd cache = true       #默认值 true      #RBD缓存
rbd cache size = 335544320       #默认值33554432           #RBD缓存大小(bytes)
rbd cache max dirty = 134217728     #默认值25165824      #缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-through
rbd cache max dirty age = 30     #默认值1                #在被刷新到存储盘前dirty数据存在缓存的时间(seconds)
rbd cache writethrough until flush = false     #默认值true  #该选项是为了兼容linux-2.6.32之前的virtio驱动，避免因为不发送flush请求，数据不回写
#设置该参数后，librbd会以writethrough的方式执行io，直到收到第一个flush请求，才切换为writeback方式。
rbd cache max dirty object = 2     #默认值0              #最大的Object对象数，默认为0，表示通过rbd cache size计算得到，librbd默认以4MB为单位对磁盘Image进行逻辑切分
#每个chunk对象抽象为一个Object；librbd中以Object为单位来管理缓存，增大该值可以提升性能
rbd cache target dirty = 235544320   #默认值16777216    #开始执行回写过程的脏数据大小，不能超过 rbd_cache_max_dirty

3. PG Number
PG和PGP数量一定要根据OSD的数量进行调整，计算公式如下，但是最后算出的结果一定要接近或者等于一个2的指数。
Total PGs = (Total_number_of_OSD * 100) / max_replication_count

例如15个OSD，副本数为3的情况下，根据公式计算的结果应该为500，最接近512，所以需要设定该pool(volumes)的pg_num和pgp_num都为512.
ceph osd pool set volumes pg_num 512
ceph osd pool set volumes pgp_num 512




mds crash
ceph mds fail 0

ceph osd perf
smartctl -A /dev/sda
filebench -f create.f  =~ 8331.53 ops

两个目录手动负载均衡测试



在测试1和测试2中都发现：虽然集群有多个MDS，但是负载严重不均衡。ceph L版本提供了一个功能，可以手动把一个目录固定到特定的MDS负载上。



 setfattr -n ceph.dir.pin -v $rank $dir



该设置可把目录$dir 固定到MDS的rank值为$rankd上



创建目录  /mnt/cephfs/test0和/mnt/cephfs/test1

并分别固定到rank为0和1的mds上：

 setfattr -n ceph.dir.pin -v 0 /mnt/cephfs/test0

 setfattr -n ceph.dir.pin -v 1 /mnt/cephfs/test1



在node01上同时执行测试程序：

  filebench-f createfiles0.f     测试目录在/mnt/cephfs/test0

  filebench-f createfiles0.f     测试目录在/mnt/cephfs/test1



测试结果

     8331.533 ops/s +  9088.984 ops/s = 17419 ops/s

 

可以看到，创建文件的总的ops 几乎线性增加。同时观察到两个MDS的负载确实都比较高。 同时观察到ceph-mds进程的CPU负载达到了 130%左右。

ceph -w
ceph pg stat, active+clean说明数据没有问题

主动维护
for i in noout nobackfill norecover; do ceph osd set $i;done
恢复将set改成unset

mds 做快照
ceph osd pool mksnap fs_meta snap1
for a in `rados -p fs_meta ls`;do echo $a >> list;done
快照回滚
for a in `cat list`;do rados -p fs_meta rollback $a snap1;done;
restart ceph-mds@

关闭NUMA
kernel numa=off

High capacity solution:
36~72 servers, each server has 4~6TB HDD

10,000 MB network; split client and cluster, or 2 1000MB 
SATA SSD as journal, intel SSD DC S3500 Series
1:4 to SATA/SAS SSD, PCIe/NVMe for 1:12 ~ 1:18
ceph-mon and ceph-mds 2G RAM, ceph-osd 1G RAM
ceph-osd and ceph-mds need more cpu resources


irqbalance 服务中断平衡
Linux预读 4096
echo "8192" > /sys/block/sda/queue/read_ahead_kb

IO调度
ssd 
echo noop > /sys/block/sda/quque/scheduler
sata 
echo deadline > /sys/block/sda/quque/scheduler



1. CephFS 安装
在已经部署好的 Ceph 集群上安装 CephFS 还是比较简单的，首先需要安装 MDS，原因是 CephFS 需要统一的元数据管理，所以 MDS 必须要有。
$ ceph-deploy --overwrite-conf mds create c7-01 c7-02 c7-03
ceph-deploy mds create {host-name}[:{daemon-name}] [{host-name}[:{daemon-name}] ...]
如果你想在同一主机上运行多个守护进程，可以为每个进程指定名字（可选）

剩下的就是 pool 相关的处理的了：
$ ceph osd pool create cephfs_data 1024
ceph osd pool create fs 10

$ ceph osd pool create cephfs_metadata 100
ceph osd pool create fs_meta 5
$ ceph fs new cephfs cephfs_metadata cephfs_data
[root@c7-01 ~]# ceph fs new cephfs fs fs_meta
new fs with metadata pool 3 and data pool 4

[root@c7-01 ~]# ceph fs ls
name: cephfs, metadata pool: fs, data pools: [fs_meta ]

[root@c7-01 ~]# ceph mds stat
cephfs-1/1/1 up  {0=c7-01=up:active}
[root@c7-01 ~]# 

client mount
ceph auth get client.admin
mount -t ceph 192.168.91.101:5678:/ /opt/fs -o user=admin,secret=xxxxxxx




## 修改默认生成的ceph.conf，增加如下配置段：
# 80G日志盘
osd_journal_size = 81920 
public_network= 192.168.20.0/24 
# 副本pg数为2，默认为3，最小工作size为默认size - (默认size/2) 
osd pool default size = 2
# 官方建议平均每个osd 的pg数量不小于30，即pg num > (osd_num) * 30 / 2(副本数)
osd pool default pg num = 1024
osd pool default pgp num = 1024


ceph-deploy install c7-{01..03} --no-adjust-repos

yum install -y yum-utils 
yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/
yum install --nogpgcheck -y epel-release
rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7
rm -f /etc/yum.repos.d/dl.fedoraproject.org*


国内源分析
以163为例，是以天为单位向回同步Ceph源，完全可以满足大多数场景的需求，同步的源也非常全，包含了calamari，debian和rpm的全部源，最近几个版本的源也能从中找到。

安装指定版本的Ceph
这里以安装最新版本的Jewel为例，由于Jewel版本中已经不提供el6的镜像源，所以只能使用CentOS 7以上版本进行安装。我们并不需要在repos里增加相应的源，只需要设置环境变量，即可让ceph-deploy使用国内源，具体过程如下：

CentOS:

export CEPH_DEPLOY_REPO_URL=http://mirrors.aliyun.com/ceph/rpm-jewel/el7
export CEPH_DEPLOY_GPG_URL=http://mirrors.aliyun.com/ceph/keys/release.asc

Ubuntu:

export CEPH_DEPLOY_REPO_URL=http://mirrors.163.com/ceph/debian-jewel
export CEPH_DEPLOY_GPG_URL=http://mirrors.163.com/ceph/keys/release.asc
 
之后的过程就没有任何区别了：

# Create monitor node
ceph-deploy new c7-01 c7-02 c7-03

# Software Installation
ceph-deploy install --release mimic  c7-01 c7-02 c7-03

# Gather keys
ceph-deploy mon create-initial

ceph-deploy admin c7-01 c7-02 c7-03
mkdir -p /data1/osd0
chown -R ceph:ceph /data1

 ceph-deploy osd prepare c7-01:/data1/osd0 c7-02:/data1/osd0  c7-03:/data1/osd0
 ceph-deploy osd activate c7-01:/data1/osd0 c7-02:/data1/osd0 c7-03:/data1/osd0

# Ceph deploy parepare and activate
ceph-deploy osd prepare node1:/dev/sdb node2:/dev/sdb node3:/dev/sdb
ceph-deploy osd activate node1:/var/lib/ceph/osd/ceph-0 node2:/var/lib/ceph/osd/ceph-1 node3:/var/lib/ceph/osd/ceph-2

创建ceph 管理进程服务
ceph-deploy mgr create c7-01

# Make 3 copies by default
echo "osd pool default size = 3" | tee -a $HOME/ceph.conf

# Copy admin keys and configuration files
ceph-deploy --overwrite-conf admin deploy node1 node2 node3

创建pool
ceph osd pool create image 64 
ceph osd pool delete rbd rbd --yes-i-really-really-mean-it  
ceph建好后默认有个rbd池，可以考虑删除

rbd create test --size 1024 -p image 
注：创建一个镜像,-p参数指定池的名称，-size单位为M
rbd create -p kube share1 --size 1M


[root@c7-01 opt]# rbd list -p kube
kubernetes-dynamic-pvc-cb79324e-56dc-11e9-b6e3-0a580af40108
share1

[root@c7-01 opt]# rbd info share1 -p kube
rbd image 'share1':
        size 1 MiB in 1 objects
        order 22 (4 MiB objects)
        id: 1bf3f6b8b4567
        block_name_prefix: rbd_data.1bf3f6b8b4567
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features: 
        flags: 
        create_timestamp: Sat Apr  6 23:14:28 2019

        可以看到foo image拥有： layering, exclusive-lock, object-map, fast-diff, deep-flatten。不过遗憾的是CentOS的3.10内核仅支持其中的layering feature，其他feature概不支持。我们需要手动disable这些features：

# rbd feature disable foo exclusive-lock, object-map, fast-diff, deep-flatten

[root@c7-01 opt]# rbd map share1 -p kube
/dev/rbd0

rbd showmapped

rbd unmap /dev/rbd0

1.3 常用操作
1.3.1 ceph reset
ceph-deploy purge 节点1 节点2 ....
ceph-deploy purgedata 节点1 节点2 ....
ceph-deploy forgetkeys
1.3.2 常用命令
rados lspools 查看池子

ceph -s 或 ceph status 查看集群状态

ceph -w 观察集群健康状态

ceph quorum_status --format json-pretty 检查ceph monitor仲裁状态

ceph df 检查集群使用情况

ceph mon stat 检查monitor状态

ceph osd stat 检查osd状态

ceph pg stat 检查pg配置组状态

ceph pg dump 列出PG

ceph osd lspools 列出存储池

ceph osd tree 检查osd的crush map

ceph auth list 列出集群的认证密钥

ceph 获取每个osd上pg的数量
